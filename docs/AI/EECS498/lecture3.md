# 线性分类器

## 上节回顾：图像分类

图像分类是将输入图像分配到一个预定义类别中的任务。

[输入为图像，输出为类别标签的示意图：显示一张猫的图像被分类到"猫"类别]

### 图像分类的挑战

识别面临许多挑战，包括：

- 光照变化
- 变形
- 遮挡
- 背景杂乱
- 类内变化
- 视角变化

[图像分类挑战示例：显示不同光照、变形、遮挡等情况的图像]

---

## 参数化方法

线性分类器是一种参数化方法，通过学习权重参数来对图像进行分类。

!!! abstract "参数化方法流程"
    - 输入：图像
    - 参数：权重W
    - 输出：各类别的分数

### 线性分类器的数学表达

$$f(x,W) = Wx + b$$

其中：
- x是输入图像（拉伸为列向量）
- W是权重矩阵
- b是偏置向量
- f(x,W)是每个类别的分数

#### 形状说明

假设有10个类别，图像为32×32×3：
- x的形状：(3072,)
- W的形状：(10, 3072)
- b的形状：(10,)
- f(x,W)的形状：(10,)

### 线性分类器的偏置技巧

可以通过在数据向量中添加一个额外的"1"，将偏置合并到权重矩阵中：

```
[ W  b ] [ x ]
          [ 1 ]
```

### 线性分类器的预测是线性的

$$f(cx, W) = W(cx) = c \cdot f(x, W)$$

这意味着将图像的像素值缩放c倍，得分也会缩放c倍。

[线性缩放示例：一张图像与其0.5倍缩放版本的得分对比]

---

## 解释线性分类器

线性分类器可以从三个角度进行解释：

### 1. 代数视角

将像素拉伸为列向量，与权重矩阵相乘。

### 2. 视觉视角

可以将W的每一行重新排列为图像大小，每个类别有一个"模板"。

!!! note "模板的局限性"
    单个模板无法捕捉数据的多种模式
    
    例如：马的模板可能会有两个头！

[线性分类器的视觉解释：显示不同类别的模板图像]

### 3. 几何视角

线性分类器在高维空间中用超平面划分不同类别的区域。

!!! example "决策区域"
    每个类别对应一个决策区域，它们之间的边界是超平面。

[几何视角示例：二维平面上的决策边界示意图]

---

## 损失函数

选择权重W的关键是定义一个损失函数来量化分类器的好坏。

!!! info "损失函数"
    损失函数告诉我们当前分类器的性能如何：
    
    - 低损失 = 好的分类器
    - 高损失 = 差的分类器
    
    也称为：目标函数、代价函数。

### 数学定义

给定一个数据集 $(x_i, y_i)_{i=1}^N$，其中：
- $x_i$ 是图像
- $y_i$ 是整数标签

单个样本的损失为 $L_i = L(f(x_i, W), y_i)$

整个数据集的损失为平均值：

$$L = \frac{1}{N}\sum_{i} L_i = \frac{1}{N}\sum_{i} L(f(x_i, W), y_i)$$

---

## 两种常见的损失函数

### 1. 交叉熵损失（多项式逻辑回归）

希望将原始分类器分数解释为概率。

#### Softmax函数

Softmax函数将原始分数转换为概率分布：

$$P(y = k | x) = \frac{\exp(s_k)}{\sum_j \exp(s_j)}$$

其中 $s = f(x_i; W)$ 是未归一化的对数概率（logits）。

#### 计算流程

1. 从分类器获得分数（例如：[3.2, 5.1, -1.7]）
2. 通过指数函数获得未归一化概率（例如：[24.5, 164.0, 0.18]）
3. 归一化概率使其总和为1（例如：[0.13, 0.87, 0.00]）
4. 计算正确类别概率的负对数

单个样本的交叉熵损失：

$$L_i = -\log(P(y = y_i | x = x_i))$$

或展开形式：

$$L_i = -\log\left(\frac{\exp(s_{y_i})}{\sum_j \exp(s_j)}\right)$$

这相当于在最大化观测数据的似然性（最大似然估计）。

!!! question "交叉熵损失的范围"
    === "问题"
        交叉熵损失的最小值和最大值是多少？
    
    === "答案"
        最小值为0（当正确类别的概率为1时）
        最大值为+∞（当正确类别的概率接近0时）

!!! question "随机初始化的损失"
    === "问题"
        如果所有分数都是小的随机值，损失是多少？
    
    === "答案"
        约为-log(1/C)，其中C是类别数
        例如，对于10个类别，约为log(10) ≈ 2.3

### 2. 多类SVM损失

SVM损失的核心思想是："正确类别的分数应该比所有其他类别的分数高。"

#### 数学定义

给定一个样本 $(x_i, y_i)$ 和分数 $s = f(x_i, W)$，SVM损失为：

$$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$$

这里的1是"边距"（margin）。

#### 计算示例

假设分数为：
- 猫：3.2（正确类别）
- 汽车：5.1
- 青蛙：-1.7

损失计算：
- 与汽车相比：max(0, 5.1 - 3.2 + 1) = max(0, 2.9) = 2.9
- 与青蛙相比：max(0, -1.7 - 3.2 + 1) = max(0, -3.9) = 0
- 总损失：2.9 + 0 = 2.9

!!! question "SVM损失的范围"
    === "问题"
        SVM损失的最小值和最大值是什么？
    
    === "答案"
        最小值为0（当正确类别的分数至少比其他类别高1时）
        最大值为+∞（理论上没有上限）

!!! question "随机初始化的损失"
    === "问题"
        如果所有分数都是随机的，预期的损失是多少？
    
    === "答案"
        对于C个类别，每个类别贡献约1的损失，总损失约为C-1

---

## 交叉熵与SVM损失的比较

假设有如下分数：
- [10, -2, 3]（第一个是正确类别）
- [10, 9, 9]（第一个是正确类别）
- [10, -100, -100]（第一个是正确类别）

!!! example "比较"
    === "SVM损失"
        对于这些例子，SVM损失都是0
        
        只要正确类别的分数足够高，SVM损失就不会变化
    
    === "交叉熵损失"
        即使正确类别的分数很高，交叉熵损失也始终为正数
        
        如果进一步增加正确类别的分数（如从10到20），交叉熵损失会继续减小

这表明交叉熵损失总是希望增加正确类别的分数，而SVM损失只关心"足够好"的分数。

---

## 总结

- 线性分类器可以从代数、视觉和几何三个角度理解
- 我们有一个数据集 $(x, y)$
- 我们有一个分数函数：$f(x; W, b) = Wx + b$
- 我们有损失函数来量化分类器的性能：
    - Softmax损失：$L_i = -\log\frac{\exp(s_{y_i})}{\sum_j \exp(s_j)}$
    - SVM损失：$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$

下一步：我们如何找到最佳的W和b？（正则化和优化）