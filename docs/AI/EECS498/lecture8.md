# CNN架构

## 引言

随着深度学习的发展，卷积神经网络(CNN)的架构设计经历了多次重大变革。自2012年AlexNet在ImageNet挑战赛上取得突破性成功以来，研究人员不断提出更深、更高效的网络结构。

[ImageNet错误率变化图：展示2010-2017年间ImageNet挑战赛中各种模型的错误率下降趋势]

---

## 批归一化（Batch Normalization）回顾

批归一化是训练深度CNN的关键技术，它解决了内部协变量偏移（internal covariate shift）问题。

### 工作原理

对于单个层 $y = Wx$，以下因素可能导致优化困难：

* 输入 $x$ 不以零为中心（需要较大的偏置）
* 输入 $x$ 的每个元素具有不同的缩放（权重 $W$ 中的元素需要大幅变化）

解决方案：在每一层强制输入进行"良好缩放"！

$$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}$$

$$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$$

其中：
* $\mu_c$ 和 $\sigma_c^2$ 是批次中每个通道的均值和方差
* $\gamma_c$ 和 $\beta_c$ 是可学习的缩放和偏移参数

[批归一化计算图：展示批归一化如何处理输入数据]

### 测试时的批归一化

在测试阶段，我们使用训练期间累积的均值和方差：

$$\mu_c^{test} = 0.99 \mu_c^{test} + 0.01 \mu_c$$

在测试时，批归一化变成一个线性操作，可以与前一个卷积或全连接层融合。

---

## AlexNet (2012)

AlexNet是第一个在ImageNet上取得重大突破的深度CNN，由Krizhevsky等人开发。

### 架构特点

* 输入尺寸：227×227
* 5个卷积层
* 最大池化
* 3个全连接层
* ReLU非线性激活函数
* 使用了局部响应归一化（现已不常用）
* 在两个GPU上训练（每个GPU只有3GB内存）

[AlexNet架构图：展示AlexNet的网络结构]

### AlexNet详细结构

| 层 | 输入尺寸 | 输出尺寸 | 参数量 | 计算量 |
|----|---------|---------|--------|--------|
| conv1 | 3×227×227 | 64×56×56 | 23K | 73M |
| pool1 | 64×56×56 | 64×27×27 | 0 | 0 |
| conv2 | 64×27×27 | 192×27×27 | 307K | 224M |
| pool2 | 192×27×27 | 192×13×13 | 0 | 0 |
| conv3 | 192×13×13 | 384×13×13 | 664K | 112M |
| conv4 | 384×13×13 | 256×13×13 | 885K | 145M |
| conv5 | 256×13×13 | 256×13×13 | 590K | 100M |
| pool5 | 256×13×13 | 256×6×6 | 0 | 0 |
| flatten | 256×6×6 | 9216 | 0 | 0 |
| fc6 | 9216 | 4096 | 37.7M | 38M |
| fc7 | 4096 | 4096 | 16.8M | 17M |
| fc8 | 4096 | 1000 | 4.1M | 4M |

### 参数与计算分布特点

* 大部分内存使用在早期卷积层
* 几乎所有参数都在全连接层中
* 大部分浮点运算发生在卷积层

[AlexNet资源分布图：展示AlexNet在内存、参数和计算量上的分布]

AlexNet在ImageNet上的Top-5错误率为16.4%，远超之前的最佳结果(25.8%)。截至2022年，该论文已被引用超过10万次。

---

## ZFNet (2013)

ZFNet是AlexNet的改进版本，由Zeiler和Fergus开发。主要改进包括：

* conv1：从11×11步长4改为7×7步长2
* conv3,4,5：滤波器数量从384,384,256改为512,1024,512

这些改进使ImageNet Top-5错误率从16.4%降至11.7%。

---

## VGG (2014)

VGG网络由牛津大学视觉几何组(Visual Geometry Group)开发，以其简洁统一的设计而闻名。

### 设计规则

* 所有卷积层均为3×3，步长1，填充1
* 所有最大池化层均为2×2，步长2
* 池化后，通道数翻倍
* 网络分为5个卷积阶段

[VGG架构图：展示VGG-16和VGG-19的网络结构]

### VGG-16与VGG-19

VGG-16包含13个卷积层和3个全连接层，共16个带参数的层：

1. 阶段1：conv-conv-pool
2. 阶段2：conv-conv-pool
3. 阶段3：conv-conv-conv-pool
4. 阶段4：conv-conv-conv-pool
5. 阶段5：conv-conv-conv-pool
6. 全连接：fc-fc-fc

VGG-19在阶段4和阶段5中各有4个卷积层。

### 设计理念

VGG使用多个小卷积核叠加代替大卷积核：

* 选项1：Conv(5×5, C→C)
  - 参数：25C²
  - 计算量：25C²HW

* 选项2：两个Conv(3×3, C→C)
  - 参数：18C²
  - 计算量：18C²HW

两个3×3卷积的感受野与一个5×5卷积相同，但参数更少，计算量更小！

### 与AlexNet对比

VGG-16比AlexNet大得多：
* 内存：48.6MB（是AlexNet的25倍）
* 参数：138M（是AlexNet的2.3倍）
* 计算量：13.6 GFLOP（是AlexNet的19.4倍）

VGG在ImageNet上的Top-5错误率为7.3%。

---

## GoogLeNet/Inception (2014)

GoogLeNet（也称为Inception-v1）专注于效率，引入了多种创新，以减少参数数量和计算量。

### 主要创新

#### 激进的Stem网络

在网络开始部分使用激进的下采样，快速减小特征图尺寸：

| 层 | 输入尺寸 | 输出尺寸 | 参数量 | 计算量 |
|----|---------|---------|--------|--------|
| conv | 3×224×224 | 64×112×112 | 9K | 118M |
| max-pool | 64×112×112 | 64×56×56 | 0 | 2M |
| conv | 64×56×56 | 64×56×56 | 4K | 13M |
| conv | 64×56×56 | 192×56×56 | 111K | 347M |
| max-pool | 192×56×56 | 192×28×28 | 0 | 1M |

从224降至28的空间分辨率：
* 内存：7.5MB（VGG-16同等部分需要42.9MB，5.7倍）
* 参数：124K（VGG-16同等部分需要1.1M，8.9倍）
* 计算量：418M（VGG-16同等部分需要7485M，17.8倍）

#### Inception模块

Inception模块是一个具有并行分支的局部单元，在整个网络中重复使用：

[Inception模块图：展示Inception模块的结构，包括1×1卷积、3×3卷积和5×5卷积的并行分支]

使用1×1"瓶颈"层减少昂贵卷积前的通道维度，以降低计算成本。

#### 全局平均池化

在网络末端不使用大型全连接层，而是使用全局平均池化塌缩空间维度，然后使用一个线性层生成类别得分：

| 层 | 输入尺寸 | 输出尺寸 | 参数量 | 计算量 |
|----|---------|---------|--------|--------|
| avg-pool | 1024×7×7 | 1024×1×1 | 0 | 0 |
| fc | 1024 | 1000 | 1025K | 1M |

相比之下，VGG-16的全连接层需要123.6M个参数和124M的计算量。

#### 辅助分类器

为了解决梯度传播问题（在批归一化出现之前），在网络中间部分附加"辅助分类器"，它们也尝试对图像进行分类并接收损失。

GoogLeNet在ImageNet上的Top-5错误率为6.7%。

---

## ResNet（残差网络）(2016)

ResNet由何凯明等人提出，解决了深度网络训练困难的问题。

### 深度网络的问题

实验表明，当网络变得非常深时，性能反而下降：

[深度网络问题图：展示56层网络比20层网络表现更差的训练和测试误差曲线]

更深的模型应该能够至少与浅层模型表现一样好（通过复制浅层模型的层并将额外层设为恒等映射）。假设：这是一个优化问题，更深的模型更难优化，特别是难以学习恒等函数。

### 残差块

解决方案：改变网络使得额外层易于学习恒等函数！

[残差块图：对比普通块和残差块的结构]

普通块：$H(x) = F(x)$
残差块：$H(x) = F(x) + x$

如果将残差块中的卷积权重设为0，整个块就会计算恒等函数！

### ResNet架构

ResNet采用堆叠多层残差块的设计：

[ResNet架构图：展示ResNet的整体结构]

* 使用与GoogLeNet相同的激进Stem网络
* 像VGG一样，使用规则设计，每个残差块有两个3×3卷积
* 网络分为几个阶段，第一个块将分辨率减半（使用步长为2的卷积）并将通道数翻倍
* 像GoogLeNet一样，末端不使用大型全连接层，而是使用全局平均池化

### 残差块类型

#### 基本残差块（Basic Block）
* 两个3×3卷积
* 总计算量：18HWC²

#### 瓶颈残差块（Bottleneck Block）
* 1×1卷积（4C→C）
* 3×3卷积（C→C）
* 1×1卷积（C→4C）
* 总计算量：17HWC²（比基本块更少的计算量！）

### ResNet变体

| 模型 | 块类型 | Stage 1 | Stage 2 | Stage 3 | Stage 4 | 计算量 | ImageNet Top-5错误率 |
|------|--------|---------|---------|---------|---------|--------|-----------------|
| ResNet-18 | 基本 | 2×2 | 2×2 | 2×2 | 2×2 | 1.8G | 10.92% |
| ResNet-34 | 基本 | 3×2 | 4×2 | 6×2 | 3×2 | 3.6G | 8.58% |
| ResNet-50 | 瓶颈 | 3×3 | 4×3 | 6×3 | 3×3 | 3.8G | 7.13% |
| ResNet-101 | 瓶颈 | 3×3 | 4×3 | 23×3 | 3×3 | 7.6G | 6.44% |
| ResNet-152 | 瓶颈 | 3×3 | 8×3 | 36×3 | 3×3 | 11.3G | 5.94% |

其中，"N×M"表示N个块，每个块有M层卷积。

ResNet-50是一个很好的基线架构，至今仍被广泛使用。

### 改进的残差块设计

后续研究提出了"预激活"残差块：

[预激活残差块图：对比原始残差块和预激活残差块的结构]

预激活设计将ReLU和BatchNorm放在残差分支内部，而不是之后，使得网络能够学习真正的恒等函数。

---

## 模型复杂度比较

不同CNN架构在准确性、参数量、计算量和内存使用上有显著差异：

[模型复杂度比较图：展示不同模型在准确率、参数量、计算量、内存使用上的关系]

* VGG：内存占用最高，计算量最大
* GoogLeNet：非常高效
* AlexNet：计算量低，参数量大
* ResNet：设计简单，效率适中，准确率高

---

## 总结

CNN架构的演变展示了深度学习设计的几个关键趋势：

* 从手工设计（AlexNet、VGG）到系统探索（ResNet）
* 从简单堆叠（VGG）到模块化设计（Inception、ResNet）
* 从参数密集（全连接层）到计算密集（多卷积层）
* 从增加深度到提高效率

现代CNN架构设计继续发展，后续会有更多创新架构出现。
