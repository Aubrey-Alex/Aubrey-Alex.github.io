# 神经网络

## 线性分类器的局限性

!!! warning "问题"
    线性分类器能力有限，无法识别复杂模式

线性分类器存在的问题：

* 从几何角度看：只能通过超平面划分空间
* 从视觉角度看：每个类别只有一个模板，无法识别一个类别的不同模式

[线性分类器局限性示意图：展示线性分类器如何使用单一模板表示每个类别]

---

## 解决方案：特征变换

特征变换可以帮助我们解决线性分类器的局限性，使其能够在变换后的特征空间中形成非线性决策边界。

### 特征变换示例

以二维空间为例，我们可以通过极坐标变换将数据从原始空间转换到特征空间：

$$r = (x^2 + y^2)^{1/2}$$
$$\theta = \tan^{-1}(y/x)$$

[特征变换过程示意图：原始空间中的点通过极坐标变换映射到特征空间]

在特征空间中使用线性分类器，相当于在原始空间中使用非线性分类器！

---

## 图像特征

### 颜色直方图

颜色直方图是一种简单的图像特征提取方法：

* 统计图像中不同颜色的分布
* 忽略了纹理和空间位置信息

[颜色直方图示例图：显示如何从图像中提取颜色分布信息]

### 方向梯度直方图 (HoG)

HoG是一种更复杂的特征提取方法：

1. 计算每个像素的边缘方向和强度
2. 将图像分为8×8的区域
3. 在每个区域内计算边缘方向的直方图，并按边缘强度加权

[HoG特征提取示意图：显示从图像计算梯度方向和强度，并按区域划分]

特点：
* 捕获纹理和位置信息
* 对小的图像变化具有鲁棒性
* 例如：320×240的图像被分为40×30个区域，每个区域8个方向，特征向量有30×40×9=10,800个数字

### 词袋模型 (Bag of Words)

词袋模型是一种数据驱动的特征提取方法：

**步骤1：构建码本**
* 提取随机图像块
* 对图像块进行聚类，形成"视觉单词"码本

**步骤2：编码图像**
* 提取图像的图像块
* 将每个图像块分配给最接近的视觉单词
* 计算视觉单词的直方图

[词袋模型示意图：展示码本构建和图像编码过程]

---

## 神经网络vs手工特征

以前的图像分类流程：

1. 特征提取（手工设计）
2. 线性分类器
3. 输出类别得分

[手工特征提取流程图：展示从输入图像到特征再到分类的流程]

神经网络的改进：

* 用神经网络替代手工设计的特征
* 端到端学习：从原始输入直接学习到输出
* 同时学习特征提取和分类

[神经网络流程图：展示神经网络如何从原始数据学习特征和分类]

---

## 神经网络结构

### 线性分类器到神经网络

**线性分类器**：
$$f(x) = Wx + b$$

其中参数：$W \in \mathbb{R}^{C \times D}$，$b \in \mathbb{R}^C$

**两层神经网络**：
$$f(x) = W_2 \max(0, W_1x + b_1) + b_2$$

其中可学习参数：
* $W_1 \in \mathbb{R}^{H \times D}$，$b_1 \in \mathbb{R}^H$
* $W_2 \in \mathbb{R}^{C \times H}$，$b_2 \in \mathbb{R}^C$

**三层神经网络**：
$$f(x) = W_3 \max(0, W_2 \max(0, W_1x + b_1) + b_2) + b_3$$

[神经网络结构示意图：表示输入层、隐藏层和输出层]

### 全连接神经网络

以一个两层神经网络为例：

* 输入：$x \in \mathbb{R}^D$（例如：3072）
* 隐藏层：$h \in \mathbb{R}^H$（例如：100）
* 输出：$s \in \mathbb{R}^C$（例如：10）

其中：
* $W_1$中的元素$(i,j)$表示输入$x_j$对隐藏层$h_i$的影响
* $W_2$中的元素$(i,j)$表示隐藏层$h_j$对输出$s_i$的影响
* 输入的所有元素影响隐藏层的所有元素
* 隐藏层的所有元素影响输出的所有元素

[全连接网络示意图：展示各层间完全连接的网络结构]

这种结构也称为**多层感知器**(MLP)。

---

## 神经网络的理解

### 从线性分类器到神经网络

线性分类器：每个类别一个模板

神经网络：
* 第一层是一组模板
* 第二层重新组合这些模板
* 可以使用不同的模板覆盖一个类别的多种模式
* "分布式表示"：大多数模板不可解释

[神经网络模板表示图：显示多层网络如何组合基本特征形成复杂表示]

### 深度神经网络

通过增加层数可以构建更深的网络：

* 深度 = 层数
* 宽度 = 每层的大小

深度网络公式：
$$f = W_6 \max(0, W_5 \max(0, W_4 \max(0, W_3 \max(0, W_2 \max(0, W_1x)))))$$

[深度网络结构图：展示具有多个隐藏层的深度网络]

---

## 激活函数

在神经网络中，激活函数用于引入非线性。常见的激活函数包括：

### ReLU（整流线性单元）

$$\text{ReLU}(x) = \max(0, x)$$

### 其他激活函数

* **Sigmoid**: $\sigma(x) = \frac{1}{1+e^{-x}}$
* **tanh**: $\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$
* **Leaky ReLU**: $f(x) = \max(0.2x, x)$
* **Softplus**: $\log(1 + \exp(x))$
* **ELU**: $f(x) = \begin{cases} x, & \text{if } x > 0 \\ \exp(x) - 1, & \text{if } x \leq 0 \end{cases}$

[激活函数对比图：显示各种激活函数的形状]

!!! tip "实用建议"
    ReLU是大多数问题的良好默认选择。

### 激活函数的重要性

如果我们构建没有激活函数的神经网络：

$$f(x) = W_2(W_1x + b_1) + b_2 = W_2W_1x + W_2b_1 + b_2$$

结果就是一个线性分类器！激活函数引入非线性，使网络能够学习复杂的函数。

---

## 神经元的生物学灵感

神经网络受到生物神经元的启发，但要谨慎使用这种类比。

### 生物神经元与人工神经元对比

生物神经元结构：
* 细胞体
* 树突（接收输入）
* 轴突（传输输出）
* 突触（连接点）

[生物神经元与人工神经元对比图：显示两者结构上的相似性]

但生物神经元比人工神经元复杂得多：
* 多种不同类型
* 树突可以执行复杂的非线性计算
* 突触不仅是单一权重，而是复杂的非线性动力系统
* 时间序列激活很重要（尖峰神经网络）

---

## 空间变形理解

神经网络如何实现非线性分类？可以通过空间变形来理解。

### 线性变换

考虑线性变换：$h = Wx$（其中$x, h$都是二维的）

[线性变换图：展示原始空间点映射到特征空间]

线性变换无法使线性不可分的点变得线性可分。

### 神经网络隐藏层变换

考虑神经网络隐藏层：$h = \text{ReLU}(Wx) = \max(0, Wx)$

ReLU激活的关键作用：
* 将部分区域"折叠"到原点
* 将其他区域"折叠"到坐标轴上
* 这种非线性折叠创造了新的分隔方式

[ReLU变换图：展示ReLU激活如何折叠空间，使原本线性不可分的点变得线性可分]

通过这种变换，原始空间中线性不可分的点在特征空间中变得线性可分！

---

## 神经网络容量与规模

神经网络的容量（表达能力）与隐藏单元数量相关：

* 更多隐藏单元 = 更大的容量

[不同隐藏单元数量的效果图：3个、6个和20个隐藏单元对比]

!!! note "规模调整建议"
    不要通过调整网络大小来正则化，而是使用更强的L2正则化。

---

## 通用近似定理

!!! abstract "通用近似定理"
    一个具有一个隐藏层的神经网络可以以任意精度近似任何函数 $f: \mathbb{R}^N \rightarrow \mathbb{R}^M$*

    *许多技术条件：仅在$\mathbb{R}^N$的紧子集上成立；函数必须是连续的；需要定义"任意精度"等。

### 二层ReLU网络近似函数示例

考虑一个近似函数 $f: \mathbb{R} \rightarrow \mathbb{R}$ 的二层ReLU网络：

* 输入：$x \in \mathbb{R}$
* 第一层权重：$w \in \mathbb{R}^{H \times 1}$
* 第一层偏置：$b \in \mathbb{R}^H$
* 第二层权重：$u \in \mathbb{R}^{1 \times H}$
* 第二层偏置：$p \in \mathbb{R}$
* 输出：$y \in \mathbb{R}$

计算过程：
$$h_i = \max(0, w_i \cdot x + b_i)$$
$$y = \sum_i u_i \cdot h_i + p$$

[二层ReLU网络示意图：展示输入、隐藏层和输出]

结果是一个偏移、缩放的ReLU之和：
$$y = \sum_i u_i \cdot \max(0, w_i \cdot x + b_i) + p$$

其中：
* 弯曲位置由$b_i$给出
* 斜率由$u_i \cdot w_i$给出
* 根据$w_i$的符号翻转左/右

通过这种方式，我们可以构建"凸起函数"，并用凸起函数之和来近似任何函数。

[凸起函数构建图：展示如何用四个隐藏单元构建一个凸起]

!!! warning "现实检查"
    虽然通用近似定理告诉我们神经网络可以表示任何函数，但它并不告诉我们：
    
    * 我们是否可以使用SGD实际学习任何函数
    * 学习一个函数需要多少数据
    
    记住：k近邻也是通用近似器！

---

## 非凸优化

神经网络通常涉及非凸优化问题。

### 凸函数和非凸函数

凸函数定义：对于所有$x_1, x_2$和$0 \leq t \leq 1$，有：
$$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2)$$

[凸函数示例图：展示$f(x) = x^2$是凸函数]

[非凸函数示例图：展示$f(x) = \sin(x)$是非凸函数]

凸函数的直观理解：一个（多维）碗。凸函数通常容易优化，可以推导出收敛到全局最小值的理论保证。

* 线性分类器（Softmax、SVM）优化的是凸函数
* 神经网络损失函数通常是非凸的

[神经网络损失景观图：显示神经网络损失函数的非凸特性，包括局部最小值、鞍点等]

大多数神经网络需要非凸优化：
* 关于收敛很少或没有保证
* 经验上似乎仍然有效
* 这是活跃的研究领域

---

## 总结

1. 特征变换 + 线性分类器允许非线性决策边界
2. 神经网络作为可学习的特征变换
3. 从线性分类器到全连接网络
4. 神经网络松散地受到生物神经元的启发，但要谨慎使用类比
5. 空间变形、通用近似、非凸优化

[总结图：展示各个关键概念的关系]

## 下一步：反向传播

我们需要计算神经网络的梯度：

$$\frac{\partial L}{\partial W_1}, \frac{\partial L}{\partial W_2}, \frac{\partial L}{\partial b_1}, \frac{\partial L}{\partial b_2}$$

然后可以使用SGD进行优化。下一讲将介绍反向传播算法。
