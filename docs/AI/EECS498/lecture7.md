# 卷积神经网络

## 全连接网络的问题

在之前的神经网络中，我们将图像拉伸成一维向量：

```
输入图像 (32×32×3) → 拉伸为向量 (3072×1)
```

[全连接层示意图：展示如何将图像拉伸成向量并进行线性变换]

!!! warning "问题"
    全连接网络不尊重图像的空间结构！

---

## 卷积神经网络的组成部分

卷积神经网络由以下组件构成：

* 卷积层（Convolution Layers）
* 池化层（Pooling Layers）
* 激活函数（Activation Functions）
* 归一化层（Normalization Layers）
* 全连接层（Fully-Connected Layers）

[卷积神经网络组件图：展示CNN的各个组成部分及其连接方式]

---

## 卷积层（Convolution Layer）

### 全连接层 vs 卷积层

**全连接层**：

* 输入：拉伸的图像向量 (3072×1)
* 权重：(10×3072)
* 输出：(10×1)，每个数字是权重行与输入向量的点积

[全连接层计算图：展示全连接层如何通过点积计算输出]

**卷积层**：

* 保留图像的空间结构：(3×32×32)
* 使用滤波器（filter）在空间上滑动，计算点积

[卷积层计算图：展示卷积滤波器如何在图像上滑动计算]

### 卷积操作详解

卷积的核心思想：滤波器在图像上滑动，计算局部区域的点积。

例如，对于3×32×32的输入图像和3×5×5的滤波器：

1. 滤波器的深度与输入图像的深度相同
2. 滤波器在空间维度上滑动
3. 每个位置计算75维的点积（3×5×5）加上偏置

[卷积操作示意图：展示滤波器如何与图像进行卷积操作]

### 激活图（Activation Maps）

每个滤波器产生一个激活图（activation map）：

* 一个3×5×5的滤波器在32×32的图像上滑动，产生28×28的输出
* 多个滤波器堆叠在一起，形成多通道的输出

例如，6个3×5×5的滤波器会产生6×28×28的输出。

[激活图堆叠示意图：展示多个滤波器如何产生多通道输出]

### 卷积层的参数

卷积层的参数包括：

* 滤波器的数量（Cout）
* 滤波器的大小（K×K）
* 步长（Stride）
* 填充（Padding）

[卷积层参数图：展示各参数如何影响卷积操作]

---

## 堆叠卷积层

当我们堆叠多个卷积层时，后面的卷积层会在前一层的输出上操作：

```
输入: 3×32×32 → 第一层: 6×28×28 → 第二层: 10×26×26 → ...
```

[堆叠卷积层示意图：展示多层卷积网络的结构和数据流]

!!! note "注意"
    如果我们只堆叠卷积层，得到的仍然是线性模型，需要在卷积层之间加入非线性激活函数。

```
输入 → 卷积 → ReLU → 卷积 → ReLU → 卷积 → ReLU → ...
```

---

## 卷积滤波器学习什么？

不同层的卷积滤波器学习不同级别的特征：

* **第一层滤波器**：学习局部图像模板，如边缘和颜色对比
* **中间层滤波器**：学习更复杂的模式和部分结构
* **高层滤波器**：学习复杂的整体模式

[卷积滤波器可视化图：展示不同层级滤波器学到的特征]

---

## 卷积的空间维度详解

### 基本卷积操作

对于输入大小W，滤波器大小K，输出大小为：

$$\text{输出大小} = W - K + 1$$

例如：7×7的输入和3×3的滤波器，产生5×5的输出。

[基本卷积操作示意图：展示7×7输入与3×3滤波器的卷积过程]

!!! warning "问题"
    特征图（feature maps）在每一层都会缩小！

### 填充（Padding）

解决方案：在输入周围添加零填充（zero padding）。

使用填充P，输出大小变为：

$$\text{输出大小} = W - K + 1 + 2P$$

[填充卷积示意图：展示添加填充后的卷积过程]

!!! tip "常用设置"
    设置 P = (K-1)/2 可以使输出大小与输入相同！

### 感受野（Receptive Field）

卷积的一个重要概念是感受野：输出中的一个元素依赖于输入中的哪些元素。

* 单个卷积层：感受野大小为K×K
* L个卷积层（每层核大小为K）：感受野大小为1 + L*(K-1)

[感受野示意图：展示多层卷积后感受野的增长]

!!! warning "问题"
    对于大图像，需要很多层才能让输出"看到"整个图像。

### 步长卷积（Strided Convolution）

解决方案：使用步长（stride）来下采样。

使用步长S，输出大小变为：

$$\text{输出大小} = \frac{W - K + 2P}{S} + 1$$

[步长卷积示意图：展示步长为2的卷积操作]

---

## 卷积层示例

**输入**：3×32×32

**卷积参数**：
* 10个5×5滤波器
* 步长为1
* 填充为2

**输出大小**：10×32×32

**可学习参数数量**：
* 每个滤波器：3×5×5 + 1 = 76（包括偏置）
* 总共：10×76 = 760

**乘加操作数量**：
* 输出元素数：10×32×32 = 10,240
* 每个输出元素：75次乘加
* 总共：75×10,240 = 768,000

---

## 1×1卷积

1×1卷积是一种特殊的卷积操作：

* 滤波器大小为1×1×C（C为输入通道数）
* 在每个空间位置执行一个C维的点积

例如：将64×56×56的输入通过32个1×1滤波器，得到32×56×56的输出。

[1×1卷积示意图：展示1×1卷积的操作过程]

!!! note "用途"
    堆叠1×1卷积层相当于在每个空间位置应用一个多层感知器（MLP）。

---

## 卷积类型

除了标准的2D卷积外，还有其他类型的卷积：

### 1D卷积

* 输入：Cin×W
* 权重：Cout×Cin×K
* 应用：序列数据、音频处理

[1D卷积示意图：展示一维卷积操作]

### 3D卷积

* 输入：Cin×H×W×D
* 权重：Cout×Cin×K×K×K
* 应用：视频处理、医学影像

[3D卷积示意图：展示三维卷积操作]

---

## 池化层（Pooling Layer）

池化层是另一种下采样方式，但不包含可学习参数：

* 在局部区域内应用池化函数（如最大值或平均值）
* 降低空间维度
* 引入平移不变性

[池化层示意图：展示池化操作如何减小特征图大小]

### 最大池化（Max Pooling）

最常用的池化操作是最大池化：

```
1 1 2 4     
5 6 7 8   →  6 8
3 2 1 0      3 4
1 2 3 4
```

[最大池化示意图：展示2×2内核、步长为2的最大池化操作]

### 池化层参数

池化层的参数包括：

* 池化窗口大小（K）
* 步长（S）
* 池化函数（最大值、平均值等）

输出大小：

$$H' = \frac{H - K}{S} + 1, \quad W' = \frac{W - K}{S} + 1$$

!!! tip "常用设置"
    * 最大池化，K=2，S=2（下采样2倍）
    * 最大池化，K=3，S=2（如AlexNet中使用）

---

## 归一化层（Normalization）

### 批归一化（Batch Normalization）

批归一化（Batch Normalization）是一种重要的归一化技术，可以加速深度网络的训练：

$$\hat{x}_{i,c} = \frac{x_{i,c} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}$$

$$y_{i,c} = \gamma_c \hat{x}_{i,c} + \beta_c$$

其中：
* $\mu_c$ 和 $\sigma_c^2$ 是批次中每个通道的均值和方差
* $\gamma_c$ 和 $\beta_c$ 是可学习的缩放和偏移参数

[批归一化示意图：展示批归一化的计算过程]

#### 批归一化的优点

* 使深度网络更容易训练
* A类更高的学习率，更快的收敛
* 对初始化更加鲁棒
* 在训练期间起到正则化作用
* 测试时可以与前一层融合，无额外开销

#### 测试时的批归一化

在测试阶段，我们使用训练期间累积的均值和方差：

$$\mu_c^{test} = 0.99 \mu_c^{test} + 0.01 \mu_c$$

在测试时，批归一化变成一个线性操作，可以与前一个卷积或全连接层融合。

### 其他归一化方法

* **层归一化（Layer Normalization）**：在特征维度上归一化
* **实例归一化（Instance Normalization）**：对每个样本的每个通道单独归一化
* **组归一化（Group Normalization）**：将通道分组并在每组内归一化

[归一化方法对比图：展示不同归一化方法的区别]

---

## 经典CNN架构：LeNet-5

LeNet-5是早期成功的CNN架构，由Yann LeCun等人在1998年提出：

```
[卷积 → ReLU → 池化] × N → 展平 → [全连接 → ReLU] × N → 全连接
```

[LeNet-5架构图：展示LeNet-5的网络结构]

### LeNet-5详细结构

| 层 | 输出大小 | 权重大小 |
|----|---------|---------|
| 输入 | 1×28×28 | - |
| 卷积 (Cout=20, K=5, P=2, S=1) | 20×28×28 | 20×1×5×5 |
| ReLU | 20×28×28 | - |
| 最大池化 (K=2, S=2) | 20×14×14 | - |
| 卷积 (Cout=50, K=5, P=2, S=1) | 50×14×14 | 50×20×5×5 |
| ReLU | 50×14×14 | - |
| 最大池化 (K=2, S=2) | 50×7×7 | - |
| 展平 | 2450 | - |
| 全连接 (2450→500) | 500 | 2450×500 |
| ReLU | 500 | - |
| 全连接 (500→10) | 10 | 500×10 |

在CNN中，通常有以下模式：

* 随着网络深入，空间尺寸减小（通过池化或步长卷积）
* 通道数增加（保持总"体积"大致不变）

---

## 总结

卷积神经网络的主要组件：

1. **卷积层**：捕获局部空间模式
2. **池化层**：减小空间维度，增加平移不变性
3. **非线性激活**：引入非线性，增强表达能力
4. **归一化层**：稳定训练，加速收敛
5. **全连接层**：通常用于最终分类

卷积网络尊重图像的空间结构，比全连接网络更适合处理图像数据。
