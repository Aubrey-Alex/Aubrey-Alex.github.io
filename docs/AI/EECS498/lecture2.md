# 图像分类

## 图像分类概述

!!! abstract "定义"
    图像分类是计算机视觉的核心任务之一，目标是将输入图像分配到预定义类别集合中的一个类别。

### 基本流程

- **输入**：图像
- **输出**：将图像分配到固定类别集合中的一个类别

[图像分类示例：输入为猫的图像，输出为"猫"标签]

---

## 图像分类的挑战

### 语义鸿沟问题

!!! info "语义鸿沟"
    计算机看到的只是一个大型数字网格（例如：800 x 600 x 3的RGB值在[0, 255]之间的矩阵），而人类看到的是有意义的物体。

### 主要挑战

1. **视角变化**
   
    当相机移动时，所有像素都会改变。
    
    [视角变化示例图：同一只猫从不同角度拍摄的图像]

2. **类内变化**
   
    同一类别的对象可能外观差异很大。
    
    [类内变化示例图：不同形态的猫]

3. **细粒度类别**
   
    需要区分非常相似的类别。
    
    [细粒度类别示例图：不同品种的猫，包括缅因猫、布偶猫、美国短毛猫]

4. **背景杂乱**
   
    目标物体可能与复杂背景混合在一起。
    
    [背景杂乱示例图：猫隐藏在复杂环境中]

5. **光照变化**
   
    不同的光照条件会极大地改变图像外观。
    
    [光照变化示例图：同一场景在不同光照条件下的变化]

6. **形变**
   
    非刚性物体可以采取多种不同的形态。
    
    [形变示例图：不同姿势的猫]

7. **遮挡**
   
    目标物体可能部分被遮挡。
    
    [遮挡示例图：部分被遮挡的猫]

---

## 图像分类的应用

### 直接应用

- 医学影像分析
- 星系分类
- 鲸鱼识别

### 作为其他任务的基础

1. **目标检测**
   
    不仅分类，还要定位图像中的多个对象。

2. **图像描述生成**
   
    生成描述图像内容的自然语言文本。

3. **其他应用**
   
    如围棋等游戏中的下一步预测。

---

## 机器学习方法

!!! note "机器学习方法"
    由于难以硬编码识别算法，我们采用数据驱动的机器学习方法：
    
    1. 收集图像和标签数据集
    2. 使用机器学习训练分类器
    3. 在新图像上评估分类器

---

## 常见图像分类数据集

### MNIST

- 10个类别：数字0-9
- 28x28灰度图像
- 50,000训练图像
- 10,000测试图像
- 被称为"计算机视觉的果蝇"
- 在MNIST上的结果通常不能推广到更复杂的数据集

### CIFAR-10

- 10个类别
- 50,000训练图像（每类5,000张）
- 10,000测试图像（每类1,000张）
- 32x32 RGB图像

### CIFAR-100

- 100个类别
- 50,000训练图像（每类500张）
- 10,000测试图像（每类100张）
- 32x32 RGB图像
- 20个超类，每个超类包含5个类别

### ImageNet

- 1,000个类别
- 约130万训练图像（每类约1,300张）
- 50,000验证图像（每类50张）
- 100,000测试图像（每类100张）
- 性能指标：Top-5准确率
- 图像大小可变，但通常调整为256x256用于训练

### Places

- 365个不同场景类型
- 约800万训练图像
- 18,250验证图像（每类50张）
- 328,500测试图像（每类900张）

### Omniglot

- 1,623个类别：来自50种不同字母表的字符
- 每类别20张图像
- 用于测试少样本学习

### 训练像素数量比较

```
MNIST:     约47M
CIFAR10:   约154M
CIFAR100:  约154M
ImageNet:  约251B
Places365: 约1.6T
```

---

## 最近邻分类器

### 基本思想

1. 记忆所有训练数据及其标签
2. 预测与测试图像最相似的训练图像的标签

### 距离度量

1. **L1距离（曼哈顿距离）**
   
    ```
    d(I1, I2) = ∑|I1(i) - I2(i)|
    ```

2. **L2距离（欧几里得距离）**
   
    ```
    d(I1, I2) = √(∑(I1(i) - I2(i))²)
    ```

### 算法复杂度

- 训练时间复杂度：O(1)
- 测试时间复杂度：O(N)

这是一个缺点：我们可以接受慢训练，但需要快速测试！

!!! tip "加速方法"
    有许多快速/近似最近邻的方法，例如 [FAISS](https://github.com/facebookresearch/faiss)

### 决策边界

最近邻分类器会在训练样本周围形成决策边界。

[决策边界图示：二维空间中的最近邻分类器，显示训练点和背景颜色表示分类区域]

- 决策边界是两个分类区域之间的边界
- 决策边界可能噪声较大，受异常值影响
- 如何平滑决策边界？使用多个邻居！

---

## K-最近邻分类器

### 基本思想

- 不是从最近邻居复制标签，而是从K个最近点进行多数投票
- 使用更多邻居有助于平滑粗糙的决策边界
- 使用更多邻居有助于减少异常值的影响
- 当K > 1时，类别之间可能会出现平局，需要某种方式打破平局

### 距离度量选择

- L1（曼哈顿）距离和L2（欧几里得）距离可能产生不同的结果
- 通过选择合适的距离度量，可以将K-最近邻应用于任何类型的数据
  - 例如：使用tf-idf相似度比较研究论文

### 超参数

K-最近邻有两个重要的超参数：
1. K值
2. 距离度量

这些是关于学习算法的选择，我们不从训练数据中学习，而是在学习过程开始时设置。选择依赖于具体问题，通常需要尝试所有可能并选择最适合数据/任务的一个。

### 设置超参数

1. **错误方法**：选择在数据上效果最好的超参数
   - 问题：K=1在训练数据上总是完美的

2. **错误方法**：将数据分成训练集和测试集，选择在测试数据上效果最好的超参数
   - 问题：无法知道算法在新数据上的表现

3. **正确方法**：将数据分成训练集、验证集和测试集；在验证集上选择超参数，在测试集上评估

4. **交叉验证**：将数据分成多个折叠，每个折叠作为验证集，平均结果
   - 适用于小数据集，但在深度学习中不太常用

---

## K-最近邻的特性

### 通用近似性

随着训练样本数量趋于无穷，最近邻可以表示任何函数！

!!! note "技术条件"
    此结论需满足多项技术条件：仅适用于紧凑域上的连续函数；需要对训练点的间距做出假设等。

### 维度灾难

!!! warning "维度灾难"
    为了均匀覆盖空间，所需训练点数量随着维度呈指数增长。
    
    - 1维空间，4个点
    - 2维空间，4²个点
    - 3维空间，4³个点
    
    32x32二值图像可能的组合数：2^(32×32) ≈ 10^308
    
    可见宇宙中基本粒子数量：约10^97

---

## K-最近邻的实际应用

### 原始像素上的局限性

在原始像素上很少使用K-最近邻，因为：
- 测试时非常慢
- 像素上的距离度量不具有信息性
  
    [示例图：原始图像与三种不同变形（方框、移位、着色）但具有相同L2距离的图像]

### 与卷积网络特征结合

使用卷积网络特征的最近邻效果很好！

例如：基于最近邻的图像描述生成

[图像描述生成示例：使用最近邻方法为图像生成描述]

---

## 总结

- 图像分类从训练图像和标签开始，必须预测测试集上的标签
- 图像分类具有挑战性，需要应对遮挡、变形、照明、类内变化等问题
- 图像分类是其他视觉任务的基础构件
- K-最近邻分类器基于最近的训练样例预测标签
- 距离度量和K是超参数
- 使用验证集选择超参数；仅在最后在测试集上运行一次！
