论⽂解读：Denoising Diffusion Probabilistic Models
DDPM是扩散模型的奠基之作，它并⾮⾸先提出扩散模型的⽂章，但是是⾸先将扩散模型⽤于图像⽣成领域的关键论⽂。本⽂将尝
试对其进⾏剖析
注意：因为我阅读本篇论⽂前并没有提前查阅专有名词的翻译，所以导致本⽂中的⼀些⽤词与常见的翻译存在偏差。下⾯给出⼀张
对照表：
正向过程：Forward process，通常译作前向过程
反向过程：Reverse process，也译作后向过程
⾼斯分布：就是正态分布。注意⽂中的⾼斯分布是多维的，因此采⽤向量的标识形式；⽐如多维标准正态分布记作
，其中 表⽰单位矩阵
建议阅读顺序：如果是第⼀次接触DDPM，可以先阅读Introduction部分，对DDPM的原理有⼀个⼤概的了解，然后再阅读前置知
识部分，补充数学基础。
前置知识：从VAE到DDPM
VAE的基本原理
可以看作是⼀张图⽚，⽽ 是⼀个（⼀些）变量（latent variable，隐变量），满⾜某种⾃定义的分布，⼀般选择标准⾼斯分布
。模型的⽬标是通过 得到 。⽽为了实现这⼀点，我们建⽴两个步骤：
Encoder： ，给定⼀张图⽚ ，⽣成⼀些维度较低的embedding 
Decoder： ，根据⾼斯分布的⼀个样本 ，⽣成图⽚
这两个部分将维度较⾼的图⽚和维度较低且满⾜常见分布的数据联系起来。VAE的⽬标就是拟合出⼀个 ，尽可能地去逼近 的
真实分布
其实VAE可以看作是DDPM的⼀个链，DDPM就相当于是把 个VAE串在了⼀起
下界
对 的期望
琴⽣不等式，可画图直观理解
这个下界还有另外⼀种展开⽅法，同样可以证明上⾯的结论：
采样
重参数化
训练集x Encoder
Mean μ
Var σ
隐变量z
标准⾼斯分布
Decoder ⽣成图x'
采样
采样
Mean μ
Var σ
隐变量z
期望的定义
散度定义
上⾯的展开结果称为ELBO，⽽我们的⽬标是最⼤化ELBO。它可以拆分为如下两个⼦⽬标：
最⼩化 prior matching term： 与其下界相差1个KL散度，它代表的是encoder拟合出的 和真实 间
的差距；当encoder完美拟合时，两者相等
最⼤化 reconstruction term：想要让decoder以最⼤的可能性从隐变量 ⽣成真实的原始数据 ，就要最⼤化
reconstruction term
因为真实分布 是未知的，所以我们最⼩化prior matching term的⽅式是：在encoder将真实数据 映射到隐变量 上时，让
尽可能满⾜某种指定的分布，通常为
VAE模型结构
下⾯的流程图展⽰了VAE的训练和⽣成过程。给定⼀张训练图 ，⾸先经过编码器将 映射到 上，拟合标准正态分布（尽可能让均值
接近0向量，⽅差接近单位向量），这样就得到了隐变量 。⽽解码器则可以根据隐变量中随机抽取的⼀个点⽣成相应的图⽚
重参重参重参重参数化数化数化数化：如果我们不采⽤重参数化技巧（如下图），因为 的采样是⼀个随机过程，包含需要优
化的参数 ，但是随机过程对参数 是不可导的
因此，我们引⼊⼀个标准正态分布 ，在这个正态分布中随机取值，将它与⽅差相乘后再与均值相加：
得到的 不改变原先分布 的均值与⽅差，并将 从随机过程中剥离出来，这样就保证了参数 可导
采样
重参数化
Mean μ
Var σ
隐变量z
标准⾼斯分布
⻢尔可夫VAE
马尔可夫VAE其实就是将多个VAE过程连在⼀起，数学性质的证明和VAE是同理的
VDM
DDPM是VDM的⼀种，只在⼀些细节上有差别。⽽VDM实际上是MHVAE（马尔可夫VAE）增加了⼀些限制条件
假设我们的训练图像为 ，中间状态为 （ ），最终状态为 。限制条件如下：
和所有隐变量 的维度相同
所有encoder 都是预先定义好的⾼斯分布模型，不需要学习（上⼀个约束，即，维度相同，保证了这⼀条约束的可
⾏性）
最终状态 是标准⾼斯分布
在DDPM中，我们⼈为定义 满⾜⾼斯分布 ，其中 是超参，⽂中是⼈为定义的，所以需
要学习的部分只有 。当然也可以通过模型学习。
我们来进⼀步理解⼀下 ， 是⼀个⼩于1的数，这意味着，每经过⼀次 ，均值
的期望都会缩⼩⼀点，更加接近0，也就离标准⾼斯分布更接近了⼀点
有了上⾯这些前置知识，你就可以较为直观地理解DDPM的基本思路了。DDPM中的公式⼏乎都可以在上⾯找到相应的原型，因此
虽然证明可能依然存在⼀定困难，但我们暂时就先不细究了，有⼀个直观的理解就可以
Introduction
本⽂提出了扩散概率模型，这是⼀个马尔可夫链（Markov chain）。包含正向过程和反向过程，正向过程（扩散过程）
是在⼀张图⽚上不断添加⾼斯噪声，有具体的表达式可以计算；⽽我们的⽬标就是估计反向过程 的概率密
度函数（PDF），这样我们就可以最终推导出我们想要的结果
马尔马尔马尔马尔可可可可夫夫夫夫链链链链：简单来说，马尔可夫链⼀个序列，这个序列的每⼀个节点的状态都只与上⼀个状态有关，那么它就是⼀个马尔克
夫链。可以理解为滞后期为1的相关序列，数学表达为：
Background
扩散模型
⽂章中将联合概率密度函数 写作 。由此，我们可以得到 的边缘概率密度函数：
反向过程
上⾯的 就是反向过程的概率分布。利⽤乘法公式和马尔可夫链的定义可以给出它的概率：
其中， 的分布为：
在反向过程中，转移概率分布函数为关于 的⾼斯分布，其均值 和协⽅差矩阵 是 的函数，它们就是我们
需要学习的参数。
乘乘乘乘法法法法公公公公式式式式
对于联合概率 ，我们有：
正向过程
正向过程（Forward process）也叫做扩散过程（diffusion process），是不断添加⾼斯噪声的过程。同样是⼀个马尔可夫链，起
点和反向过程相反，为 。我们不难给出正向过程的条件概率：
满⾜如下的概率分布：
其中 是添加⾼斯噪声的⽅差表（variance schedule）， 是单位矩阵
注意到，论⽂在反向过程给出的是 ，⽽正向过程给出的则是 。是因为在正向过程中，初始图像 是已知的，给
出单步转移概率更能体现噪声的添加过程
最⼤似然
训练模型的过程是对 最⼤似然估计进⾏优化的过程。具体来说，优化的是usual variational bound on negative log 
likelihood
优化的⽬标是让 的期望最⼤，即 的期望最⼩（采⽤对数是因为对数可以将乘法降维为加法，简化求导且不会影
响结果）：
重写
前向过程有⼀个重要的性质，就是在任意时刻 ，都有：
其中 ，
然后，我们将 改写为（推导不难，暂时先不细究）：
其中， 是Kullback-Leibler散度，⽤于衡量两个概率分布之间的差异，定义为：
也就是说， 中的 ⽐较的是反向过程的 和正向过程的后验（forward process posteriors），当给定 时，
不难给出：
其中
也是⾼斯分布，因此，所有KL散度都是⾼斯间的⽐较，可以⽤闭表达式的Rao-Blackwellized计算（暂时先不细究）
我们对照下图来直观理解⼀下这个公式：
左上⽅的 对应的是上⾯公式中的 （由 到 的最后⼀步）；
右下⾓的 对应的是上⾯公式中的 （虽然写法不⼀样，但本质上就是⽐较 和 ）；
剩下的所有箭头都是 ⾥的项，这也是我们优化的重⼼
注意到，当 时， 就是隐变量 。 项消失， 就是VAE中的prior matching term，⽽ 就是VAE中的
reconstruction term。VAE实际上就是DDPM的⼀种特殊情况
Diffusion Models and Denoising Autoencoders 扩散模型和⾃动降噪编码
Diffusion Models and Denoising Autoencoders 扩散模型和⾃动降噪编码
器
⾮常好，我们终于把背景看完了。虽然从数学上来看，扩散模型似乎已经⾮常明确且唯⼀，但在⼯程实践中，还有⾮常⼤的⾃由度。
⽐如，我们需要选择正向过程的⽅差 和反向过程的模型结构等
和
为定为定为定为定值值值值：因为我们将正向过程的⽅差 定为常量，因此 是确定的，没有可学习的参数。所以 是定值，没有优化空间
优优优优化化化化空空空空间较⼩ 间较⼩ 间较⼩ 间较⼩：⽽ 虽然有优化空间，但因为只有⼀项，所以实际上优化效果并不明显，因此也不是我们优化的重点
反向过程和
想要优化 ，有两个需要考虑的分布—— 和 。前者是⼈为确定的，所以我们的任务就是优化
，让它尽可能接近 。其中 满⾜如下分布：
⽅差
我们将 定为未训练的时间相关的常数，实验表明，取 或 的结果是类似的。选择前
者在 情况下最优，后者在 为某确定的点时最优
均值
我们先对 进⾏改写：
其中 是⼀个常数，不可优化。因此我们的优化⽬标是让 尽可能地接近
上⾯我们提到 ，我们对 进⾏重参数化，让 （
为标准正态分布 ）：
训练算法
为了让 尽可能⼩，我们需要让 尽可能拟合 。因为 是给定的，因此它可以作为模型的输⼊。我
们选择如下重参数化：
所以实际上我们需要学习的参数就是 ，也就是下⾯Algorithm 2的学习过程。因为
（ ），loss可以进⼀步化简：
这个式⼦就代表了图像⽣成的质量。因为前⾯的常数系数没什么⽤，所以可以进⼀步化简为
Algorithm 1 Training
1. 循环
2. 采样原始图像
3. 采样时间步
4. 增加噪声
5. 计算损失，更新模型
6. 直直直直到到到到收敛
Algorithm 2 Sampling
1. 得到⼀个⾼斯噪声
2. for do
当 时， ，否则
3. end for
4. return
