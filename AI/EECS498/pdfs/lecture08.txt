Justin Johnson February 2, 2022
Lecture 8:
CNN Architectures
Lecture 8 - 1
Justin Johnson February 2, 2022
Assignment 3
Lecture 8 - 2
Assignment 3 is released! It covers:
• Fully-connected networks
• Dropout
• Update rules: SGD+Momentum, RMSprop, Adam
• Convolutional networks
• Batch normalization
Due Friday February 11, 11:59pm ET
Justin Johnson February 2, 2022
Last Time: Components of Convolutional Networks
Lecture 8 - 3
Convolution Layers Pooling Layers
x h s
Fully-Connected Layers
Activation Function Normalization
?
!
!,# =
?!,# − ?#
?#
$ + ?
Justin Johnson February 2, 2022
Last Time: Components of Convolutional Networks
Lecture 8 - 4
Convolution Layers Pooling Layers
x h s
Fully-Connected Layers
Activation Function Normalization
?
!
!,# =
?!,# − ?#
?#
$ + ?
Justin Johnson February 2, 2022
Batch Normalization
Lecture 8 - 5
Consider a single layer ? = ??
The following could lead to tough optimization:
- Inputs x are not centered around zero (need large bias)
- Inputs x have different scaling per-element 
(entries in W will need to vary a lot)
Idea: force inputs to be “nicely scaled” at each layer!
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 6
Ioffe and Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, ICML 2015
Idea: “Normalize” the inputs of a layer so they have zero mean and 
unit variance
We can normalize a batch of activations like this:
This is a differentiable function, so 
we can use it as an operator in our 
networks and backprop through it!
?
!
=
? − ? ?
??? ?
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 7
Ioffe and Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, ICML 2015
Input:
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
X N
D
Per-channel 
std, shape is D
? ∈ ℝ!×# ?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 8
Ioffe and Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, ICML 2015
Input:
X N
D Problem: What if zero-mean, unit 
variance is too hard of a constraint? 
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
? ∈ ℝ!×#
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 9
Learnable scale and 
shift parameters:
Output,
Shape is N x D
Learning ? = ?, ? = ?
will recover the identity 
function (in expectation)
Input:
?!,# = ?#
!
?!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
? ∈ ℝ!×#
?, ? ∈ ℝ#
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 10
Learnable scale and 
shift parameters:
Output,
Shape is N x D
Learning ? = ?, ? = ?
will recover the identity 
function (in expectation)
Input:
?!,# = ?#
!
?!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
? ∈ ℝ!×#
?, ? ∈ ℝ#
Problem: Estimates depend on 
minibatch; can’t do this at test-time!
Justin Johnson February 2, 2022
Batch Normalization: Test-Time
Lecture 7 - 11
Learnable scale and 
shift parameters:
Input:
Learning ? = ?, ? = ?
will recover the identity 
function (in expectation)
? ∈ ℝ!×#
?, ? ∈ ℝ#
Output,
Shape is N x D
?!,# = ?#
!
?!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
(Running) average of 
values seen during 
training
(Running) average of 
values seen during training
Justin Johnson February 2, 2022
Batch Normalization: Test-Time
Lecture 7 - 12
Learnable scale and 
shift parameters:
Input:
Learning ? = ?, ? = ?
will recover the identity 
function (in expectation)
? ∈ ℝ!×#
?, ? ∈ ℝ#
Output,
Shape is N x D
?!,# = ?#
!
?!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
?
1 -!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
(Running) average of 
values seen during 
training
(Running) average of 
values seen during training
?!
"#$" = 0
For each training iteration:
?! =
%
& ∑'
&
(% ?',!
?!
"#$" = 0.99 ?!
"#$" + 0.01 ?!
(Similar for ?)
Justin Johnson February 2, 2022
Batch Normalization: Test-Time
Lecture 7 - 13
Learnable scale and 
shift parameters:
Input:
Learning ? = ?, ? = ?
will recover the identity 
function (in expectation)
? ∈ ℝ!×#
?, ? ∈ ℝ#
Output,
Shape is N x D
?!,# = ?#
!
?!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
(Running) average of 
values seen during 
training
(Running) average of 
values seen during training
Justin Johnson February 2, 2022
Batch Normalization: Test-Time
Lecture 7 - 14
Learnable scale and 
shift parameters:
Input: ? ∈ ℝ!×#
?, ? ∈ ℝ#
Output,
Shape is N x D
?!,# = ?#?
!
!,# + ?#
Per-channel 
mean, shape is D
Normalized x,
Shape is N x D
Per-channel 
std, shape is D
?# =
1
?-!
'
%&
?!,#
?#
$ =
1
?-!
'
%&
?!,# − ?#
$
!
?!,# =
?!,# − ?#
?#
$ + ?
(Running) average of 
values seen during 
training
(Running) average of 
values seen during training
During testing batchnorm
becomes a linear operator! 
Can be fused with the previous 
fully-connected or conv layer
Justin Johnson February 2, 2022
? ∶ ? × ?
?, ? ∶ 1 × ?
?, ? ∶ 1 × ?
? =
? − ?
?
? + ?
Batch Normalization for ConvNets
Lecture 7 - 15
? ∶ ? × ? × ? × ?
?, ? ∶ 1 × ? × 1 × 1
?, ? ∶ 1 × ? × 1 × 1
? =
? − ?
?
? + ?
Normalize
Batch Normalization for 
fully-connected networks
Batch Normalization for 
convolutional networks
(Spatial Batchnorm, BatchNorm2D)
Normalize
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 16
FC
BN
tanh
FC
BN
tanh
Usually inserted after Fully Connected 
or Convolutional layers, and before 
nonlinearity.
Ioffe and Szegedy, “Batch normalization: Accelerating deep 
network training by reducing internal covariate shift”, ICML 2015
?
!
=
? − ? ?
??? ?
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 17
FC
BN
tanh
FC
BN
tanh
- Makes deep networks much easier to train!
- Allows higher learning rates, faster convergence
- Networks become more robust to initialization
- Acts as regularization during training
- Zero overhead at test-time: can be fused with conv!
Training iterations
ImageNet 
accuracy
Ioffe and Szegedy, “Batch normalization: Accelerating deep 
network training by reducing internal covariate shift”, ICML 2015
Justin Johnson February 2, 2022
Batch Normalization
Lecture 7 - 18
FC
BN
tanh
FC
BN
tanh
- Makes deep networks much easier to train!
- Allows higher learning rates, faster convergence
- Networks become more robust to initialization
- Acts as regularization during training
- Zero overhead at test-time: can be fused with conv!
- Not well-understood theoretically (yet)
- Behaves differently during training and testing: this 
is a very common source of bugs!
Ioffe and Szegedy, “Batch normalization: Accelerating deep 
network training by reducing internal covariate shift”, ICML 2015
Justin Johnson February 2, 2022
? ∶ ? × ?
?, ? ∶ 1 × ?
?, ? ∶ 1 × ?
? =
? − ?
?
? + ?
? ∶ ? × ?
?, ? ∶ ? × 1
?, ? ∶ 1 × ?
? =
? − ?
?
? + ?
Layer Normalization
Lecture 7 - 19
Normalize
Batch Normalization for 
fully-connected networks
Normalize
Layer Normalization for fully￾connected networks
Same behavior at train and test!
Used in RNNs, Transformers
Justin Johnson February 2, 2022
Instance Normalization
Lecture 7 - 20
? ∶ ? × ? × ? × ?
?, ? ∶ 1 × ? × 1 × 1
?, ? ∶ 1 × ? × 1 × 1
? =
? − ?
?
? + ?
Batch Normalization for 
convolutional networks
Normalize
? ∶ ? × ? × ? × ?
?, ? ∶ ? × ? × 1 × 1
?, ? ∶ 1 × ? × 1 × 1
? =
? − ?
?
? + ?
Instance Normalization for 
convolutional networks
Normalize
Justin Johnson February 2, 2022
Comparison of Normalization Layers
Lecture 7 - 21
Wu and He, “Group Normalization”, ECCV 2018
Justin Johnson February 2, 2022
Group Normalization
Lecture 7 - 22
Wu and He, “Group Normalization”, ECCV 2018
Justin Johnson February 2, 2022
Components of Convolutional Networks
Lecture 8 - 23
Convolution Layers Pooling Layers
x h s
Fully-Connected Layers
Activation Function Normalization
!
?!,# =
?!,# − ?#
?#
$ + ?
Question: How 
should we put 
them together?
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 24
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 25
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 26
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
227 x 227 inputs
5 Convolutional layers
Max pooling
3 fully-connected layers
ReLU nonlinearities
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 27
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
227 x 227 inputs
5 Convolutional layers
Max pooling
3 fully-connected layers
ReLU nonlinearities
Used “Local response normalization”;
Not used anymore
Trained on two GTX 580 GPUs – only 
3GB of memory each! Model split 
over two GPUs
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 28
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 29
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Total Citations: 102,486
333 1018
2816
6206
10647
16089
20092 21265 20912
0
5000
10000
15000
20000
25000
2013 2014 2015 2016 2017 2018 2019 2020 2021
AlexNet Citations per year
(as of 2/2/2022)
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 30
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Citation Counts
Darwin, “On the origin of species”, 1859: 60,117
Shannon, “A mathematical theory of 
communication”, 1948: 140,459
Watson and Crick, “Molecular Structure of 
Nucleic Acids”, 1953: 16,298 333 1018
2816
6206
10647
16089
20092 21265 20912
0
5000
10000
15000
20000
25000
2013 2014 2015 2016 2017 2018 2019 2020 2021
AlexNet Citations per year
(as of 2/2/2022)
Total Citations: 102,486
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 31
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 32
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Recall: Output channels = number of filters
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 33
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Recall: W’ = (W – K + 2P) / S + 1
= 227 – 11 + 2*2) / 4 + 1
= 220/4 + 1 = 56
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 34
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 35
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Number of output elements = C * H’ * W’ 
= 64*56*56 = 200,704
Bytes per element = 4 (for 32-bit floating point)
KB = (number of elements) * (bytes per elem) / 1024
= 200704 * 4 / 1024
= 784
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 36
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 37
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Weight shape = Cout x Cin x K x K
= 64 x 3 x 11 x 11
Bias shape = Cout = 64
Number of weights = 64*3*11*11 + 64
= 23,296
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 38
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 39
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Number of floating point operations (multiply+add)
= (number of output elements) * (ops per output elem)
= (Cout x H’ x W’) * (Cin x K x K)
= (64 * 56 * 56) * (3 * 11 * 11)
= 200,704 * 363
= 72,855,552
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 40
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 41
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
For pooling layer:
#output channels = #input channels = 64
W’ = floor((W – K) / S + 1)
= floor(53 / 2 + 1) = floor(27.5) = 27
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 42
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
#output elems = Cout x H’ x W’
Bytes per elem = 4
KB = Cout * H’ * W’ * 4 / 1024
= 64 * 27 * 27 * 4 / 1024
= 182.25
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 43
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Pooling layers have no learnable parameters!
?
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 44
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36
flatten 256 6 9216 36
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Floating-point ops for pooling layer
= (number of output positions) * (flops per output position)
= (Cout * H’ * W’) * (K * K)
= (64 * 27 * 27) * (3 * 3)
= 419,904
= 0.4 MFLOP
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 45
Figure copyright Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, 2012. Reproduced with permission. 
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127 0 0
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36 0 0
flatten 256 6 9216 36 0 0
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Flatten output size = Cin x H x W
= 256 * 6 * 6
= 9216
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 46
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127 0 0
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36 0 0
flatten 256 6 9216 36 0 0
fc6 9216 4096 4096 16 37,726 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
FC params = Cin * Cout + Cout
= 9216 * 4096 + 4096
= 37,725,832
FC flops = Cin * Cout
= 9216 * 4096
= 37,748,736
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 47
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127 0 0
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36 0 0
flatten 256 6 9216 36 0 0
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 48
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127 0 0
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36 0 0
flatten 256 6 9216 36 0 0
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
How to choose this?
Trial and error =(
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 49
Input size Layer Output size
Layer C H / W filters kernel stride pad C H / W memory (KB) params (k) flop (M)
conv1 3 227 64 11 4 2 64 56 784 23 73
pool1 64 56 3 2 0 64 27 182 0 0
conv2 64 27 192 5 1 2 192 27 547 307 224
pool2 192 27 3 2 0 192 13 127 0 0
conv3 192 13 384 3 1 1 384 13 254 664 112
conv4 384 13 256 3 1 1 256 13 169 885 145
conv5 256 13 256 3 1 1 256 13 169 590 100
pool5 256 13 3 2 0 256 6 36 0 0
flatten 256 6 9216 36 0 0
fc6 9216 4096 4096 16 37,749 38
fc7 4096 4096 4096 16 16,777 17
fc8 4096 1000 1000 4 4,096 4
Interesting trends here!
Justin Johnson February 2, 2022
AlexNet
Lecture 8 - 50
0
5000
10000
15000
20000
25000
30000
35000
40000
Params (K)
0
50
100
150
200
250
MFLOP
0
100
200
300
400
500
600
700
800
900
Memory (KB)
Most of the memory 
usage is in the early 
convolution layers
Nearly all parameters are in 
the fully-connected layers
Most floating-point 
ops occur in the 
convolution layers
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 51
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 52
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ZFNet: A Bigger AlexNet
Lecture 8 - 53
AlexNet but:
CONV1: change from (11x11 stride 4) to (7x7 stride 2)
CONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512
More trial and error =(
ImageNet top 5 error: 16.4% -> 11.7%
Zeiler and Fergus, “Visualizing and Understanding Convolutional Networks”, ECCV 2014
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 54
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 55
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 56
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 57
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Network has 5 convolutional stages:
Stage 1: conv-conv-pool
Stage 2: conv-conv-pool
Stage 3: conv-conv-pool
Stage 4: conv-conv-conv-[conv]-pool
Stage 5: conv-conv-conv-[conv]-pool
(VGG-19 has 4 conv in stages 4 and 5)
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 58
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Option 1:
Conv(5x5, C -> C)
Params: 25C2
FLOPs: 25C2HW
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 59
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Option 1:
Conv(5x5, C -> C)
Params: 25C2
FLOPs: 25C2HW
Option 2:
Conv(3x3, C -> C)
Conv(3x3, C -> C)
Params: 18C2
FLOPs: 18C2HW
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 60
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Option 1:
Conv(5x5, C -> C)
Params: 25C2
FLOPs: 25C2HW
Option 2:
Conv(3x3, C -> C)
Conv(3x3, C -> C)
Params: 18C2
FLOPs: 18C2HW
Two 3x3 conv has same 
receptive field as a single 5x5 
conv, but has fewer parameters 
and takes less computation!
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 61
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Input: C x 2H x 2W
Layer: Conv(3x3, C->C)
Memory: 4HWC
Params: 9C2
FLOPs: 36HWC2
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 62
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Input: C x 2H x 2W
Layer: Conv(3x3, C->C)
Memory: 4HWC
Params: 9C2
FLOPs: 36HWC2
Input: 2C x H x W
Conv(3x3, 2C -> 2C)
Memory: 2HWC
Params: 36C2
FLOPs: 36HWC2
Justin Johnson February 2, 2022
VGG: Deeper Networks, Regular Design
Lecture 8 - 63
3x3 conv, 128
Pool
3x3 conv, 64
3x3 conv, 64
Input
3x3 conv, 128
Pool
3x3 conv, 256
3x3 conv, 256
Pool
3x3 conv, 512
3x3 conv, 512
Pool
3x3 conv, 512
3x3 conv, 512
Pool
FC 4096
FC 1000
Softmax
FC 4096
3x3 conv, 512
3x3 conv, 512
3x3 conv, 384
Pool
5x5 conv, 256
11x11 conv, 96
Input
Pool
3x3 conv, 384
3x3 conv, 256
Pool
FC 4096
FC 4096
Softmax
FC 1000
Pool
Input
Pool
Pool
Pool
Pool
Softmax
3x3 conv, 512
3x3 conv, 512
3x3 conv, 256
3x3 conv, 256
3x3 conv, 128
3x3 conv, 128
3x3 conv, 64
3x3 conv, 64
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
FC 4096
FC 1000
FC 4096
AlexNet VGG16 VGG19
Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015
VGG Design rules:
All conv are 3x3 stride 1 pad 1
All max pool are 2x2 stride 2
After pool, double #channels
Input: C x 2H x 2W
Layer: Conv(3x3, C->C)
Memory: 4HWC
Params: 9C2
FLOPs: 36HWC2
Input: 2C x H x W
Conv(3x3, 2C -> 2C)
Memory: 2HWC
Params: 36C2
FLOPs: 36HWC2
Conv layers at each spatial 
resolution take the same 
amount of computation!
Justin Johnson February 2, 2022
AlexNet vs VGG-16: Much bigger network!
Lecture 8 - 64
0
5000
10000
15000
20000
25000
30000
AlexNet vs VGG-16
(Memory, KB)
AlexNet VGG-16
0
20000
40000
60000
80000
100000
120000
AlexNet vs VGG-16 
(Params, M)
AlexNet VGG-16
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
AlexNet vs VGG-16 
(MFLOPs)
AlexNet VGG-16
AlexNet total: 1.9 MB
VGG-16 total: 48.6 MB (25x)
AlexNet total: 61M
VGG-16 total: 138M (2.3x)
AlexNet total: 0.7 GFLOP
VGG-16 total: 13.6 GFLOP (19.4x)
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 65
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 66
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
GoogLeNet: Focus on Efficiency
Lecture 8 - 67
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Many innovations for efficiency: reduce parameter 
count, memory usage, and computation
Justin Johnson February 2, 2022
GoogLeNet: Aggressive Stem
Lecture 8 - 68
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Stem network at the start aggressively downsamples input
(Recall in VGG-16: Most of the compute was at the start)
Justin Johnson February 2, 2022
GoogLeNet: Aggressive Stem
Lecture 8 - 69
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Input size Layer Output size
Layer C H / W filters kernelstride pad C H/W memory (KB) params (K)flop (M)
conv 3 224 64 7 2 3 64 112 3136 9 118
max-pool 64 112 3 2 1 64 56 784 0 2
conv 64 56 64 1 1 0 64 56 784 4 13
conv 64 56 192 3 1 1 192 56 2352 111 347
max-pool 192 56 3 2 1 192 28 588 0 1
Total from 224 to 28 spatial resolution:
Memory: 7.5 MB
Params: 124K
MFLOP: 418
Stem network at the start aggressively downsamples input
(Recall in VGG-16: Most of the compute was at the start)
Justin Johnson February 2, 2022
GoogLeNet: Aggressive Stem
Lecture 8 - 70
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Input size Layer Output size
Layer C H / W filters kernelstride pad C H/W memory (KB) params (K)flop (M)
conv 3 224 64 7 2 3 64 112 3136 9 118
max-pool 64 112 3 2 1 64 56 784 0 2
conv 64 56 64 1 1 0 64 56 784 4 13
conv 64 56 192 3 1 1 192 56 2352 111 347
max-pool 192 56 3 2 1 192 28 588 0 1
Total from 224 to 28 spatial resolution:
Memory: 7.5 MB
Params: 124K
MFLOP: 418
Compare VGG-16:
Memory: 42.9 MB (5.7x)
Params: 1.1M (8.9x)
MFLOP: 7485 (17.8x)
Stem network at the start aggressively downsamples input
(Recall in VGG-16: Most of the compute was at the start)
Justin Johnson February 2, 2022
GoogLeNet: Inception Module
Lecture 8 - 71
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Inception module
Local unit with
parallel branches
Local structure repeated
many times throughout the 
network
Justin Johnson February 2, 2022
GoogLeNet: Inception Module
Lecture 8 - 72
Szegedy et al, “Going deeper with convolutions”, CVPR 2015
Inception module
Local unit with
parallel branches
Local structure repeated
many times throughout the 
network
Uses 1x1 “Bottleneck” 
layers to reduce channel 
dimension before 
expensive conv (we will 
revisit this with ResNet!)
Justin Johnson February 2, 2022
GoogLeNet: Global Average Pooling
Lecture 8 - 73
No large FC layers at the end! Instead uses global average pooling to 
collapse spatial dimensions, and one linear layer to produce class scores
(Recall VGG-16: Most parameters were in the FC layers!)
Input size Layer Output size
Layer C H/W filters kernel stride pad C H/W memory (KB) params (k) flop (M)
avg-pool 1024 7 7 1 0 1024 1 4 0 0
fc 1024 1000 1000 0 1025 1
Layer C H/W filters kernel stride pad C H/W memory (KB) params (K) flop (M)
flatten 512 7 25088 98
fc6 25088 4096 4096 16 102760 103
fc7 4096 4096 4096 16 16777 17
fc8 4096 1000 1000 4 4096 4
Compare with VGG-16:
Justin Johnson February 2, 2022
GoogLeNet: Global Average Pooling
Lecture 8 - 74
No large FC layers at the end! Instead uses global average pooling to 
collapse spatial dimensions, and one linear layer to produce class scores
(Recall VGG-16: Most parameters were in the FC layers!)
Input size Layer Output size
Layer C H/W filters kernel stride pad C H/W memory (KB) params (k) flop (M)
avg-pool 1024 7 7 1 0 1024 1 4 0 0
fc 1024 1000 1000 0 1025 1
Layer C H/W filters kernel stride pad C H/W memory (KB) params (K) flop (M)
flatten 512 7 25088 98
fc6 25088 4096 4096 16 102760 103
fc7 4096 4096 4096 16 16777 17
fc8 4096 1000 1000 4 4096 4
Compare with VGG-16:
Justin Johnson February 2, 2022
GoogLeNet: Auxiliary Classifiers
Lecture 8 - 75
Training using loss at the end of the network didn’t work well:
Network is too deep, gradients don’t propagate cleanly
As a hack, attach “auxiliary classifiers” at several intermediate points 
in the network that also try to classify the image and receive loss
GoogLeNet was before batch normalization! With BatchNorm no 
longer need to use this trick
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 76
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 77
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 78
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Once we have Batch Normalization, we can train networks with 10+ layers. 
What happens as we go deeper?
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 79
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Once we have Batch Normalization, we can train networks with 10+ layers. 
What happens as we go deeper?
Deeper model does worse than 
shallow model!
Initial guess: Deep model is 
overfitting since it is much 
bigger than the other model Iterations
56-layer
20-layer
Test error
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 80
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Once we have Batch Normalization, we can train networks with 10+ layers. 
What happens as we go deeper?
Training error
Iterations
56-layer
20-layer
Iterations
56-layer
20-layer
Test error
In fact the deep model seems to be underfitting since it also performs worse 
than the shallow model on the training set! It is actually underfitting
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 81
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
A deeper model can emulate a shallower model: copy layers from 
shallower model, set extra layers to identity
Thus deeper models should do at least as good as shallow models
Hypothesis: This is an optimization problem. Deeper models are 
harder to optimize, and in particular don’t learn identity functions to 
emulate shallow models
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 82
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
A deeper model can emulate a shallower model: copy layers from 
shallower model, set extra layers to identity
Thus deeper models should do at least as good as shallow models
Hypothesis: This is an optimization problem. Deeper models are 
harder to optimize, and in particular don’t learn identity functions to 
emulate shallow models
Solution: Change the network so learning identity functions with 
extra layers is easy!
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 83
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
conv
conv
relu
“Plain” block
X
H(x)
relu
Residual Block
conv
conv
Additive 
“shortcut”
F(x) + x
F(x)
relu
X
Solution: Change the network so learning identity functions with extra layers is easy!
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 84
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
conv
conv
relu
“Plain” block
X
H(x)
relu
Residual Block
conv
conv
Additive 
“shortcut”
F(x) + x
F(x)
relu
X
Solution: Change the network so learning identity functions with extra layers is easy!
If you set these to 
0, the whole block 
will compute the 
identity function!
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 85
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
relu
Residual block
3x3 conv
3x3 conv
F(x) + x
F(x)
relu
X
A residual network is a stack of 
many residual blocks
Regular design, like VGG: each 
residual block has two 3x3 conv
Network is divided into stages: the 
first block of each stage halves the 
resolution (with stride-2 conv) and 
doubles the number of channels
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 86
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
Uses the same aggressive stem as GoogleNet to 
downsample the input 4x before applying residual blocks:
Input 
size Layer
Output 
size
Layer C H/W filters kernel stride pad C H/W memory (KB)
params 
(k)
flop 
(M)
conv 3 224 64 7 2 3 64 112 3136 9 118
max-pool 64 112 3 2 1 64 56 784 0 2
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 87
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
Like GoogLeNet, no big fully-connected-layers: instead use 
global average pooling and a single linear layer at the end
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 88
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
ResNet-18:
Stem: 1 conv layer
Stage 1 (C=64): 2 res. block = 4 conv
Stage 2 (C=128): 2 res. block = 4 conv
Stage 3 (C=256): 2 res. block = 4 conv
Stage 4 (C=512): 2 res. block = 4 conv
Linear
ImageNet top-5 error: 10.92 
GFLOP: 1.8
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 89
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
ResNet-18:
Stem: 1 conv layer
Stage 1 (C=64): 2 res. block = 4 conv
Stage 2 (C=128): 2 res. block = 4 conv
Stage 3 (C=256): 2 res. block = 4 conv
Stage 4 (C=512): 2 res. block = 4 conv
Linear
ImageNet top-5 error: 10.92 
GFLOP: 1.8
ResNet-34:
Stem: 1 conv layer
Stage 1: 3 res. block = 6 conv
Stage 2: 4 res. block = 8 conv
Stage 3: 6 res. block = 12 conv
Stage 4: 3 res. block = 6 conv
Linear
ImageNet top-5 error: 8.58 
GFLOP: 3.6
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 90
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
ResNet-18:
Stem: 1 conv layer
Stage 1 (C=64): 2 res. block = 4 conv
Stage 2 (C=128): 2 res. block = 4 conv
Stage 3 (C=256): 2 res. block = 4 conv
Stage 4 (C=512): 2 res. block = 4 conv
Linear
ImageNet top-5 error: 10.92 
GFLOP: 1.8
ResNet-34:
Stem: 1 conv layer
Stage 1: 3 res. block = 6 conv
Stage 2: 4 res. block = 8 conv
Stage 3: 6 res. block = 12 conv
Stage 4: 3 res. block = 6 conv
Linear
ImageNet top-5 error: 8.58 
GFLOP: 3.6
VGG-16:
ImageNet top-5 error: 9.62
GFLOP: 13.6
Justin Johnson February 2, 2022
Residual Networks: Basic Block
Lecture 8 - 91
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
“Basic”
Residual block
Conv(3x3, C->C)
Conv(3x3, C->C)
Justin Johnson February 2, 2022
Residual Networks: Basic Block
Lecture 8 - 92
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
“Basic”
Residual block
Conv(3x3, C->C)
Conv(3x3, C->C) FLOPs: 9HWC2
FLOPs: 9HWC2
Total FLOPs:
18HWC2
Justin Johnson February 2, 2022
Residual Networks: Bottleneck Block
Lecture 8 - 93
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
“Basic”
Residual block
Conv(3x3, C->C)
Conv(3x3, C->C)
Conv(1x1, 4C->C)
Conv(3x3, C->C)
Conv(1x1, C->4C)
FLOPs: 9HWC2
FLOPs: 9HWC2
Total FLOPs:
18HWC2 “Bottleneck”
Residual block
Justin Johnson February 2, 2022
Residual Networks: Bottleneck Block
Lecture 8 - 94
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
“Basic”
Residual block
Conv(3x3, C->C)
Conv(3x3, C->C)
Conv(1x1, 4C->C)
Conv(3x3, C->C)
Conv(1x1, C->4C)
FLOPs: 9HWC2
FLOPs: 9HWC2
FLOPs: 4HWC2
FLOPs: 9HWC2
FLOPs: 4HWC2
Total FLOPs:
18HWC2 Total FLOPs:
17HWC2
“Bottleneck”
Residual block
More layers, less computational cost!
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 95
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
Stage 1 Stage 2 Stage 3 Stage 4
Block 
type
Stem 
layers Blocks Layers Blocks Layers Blocks Layers Blocks Layers
FC 
layers GFLOP
ImageNet 
top-5 error
ResNet-18 Basic 1 2 4 2 4 2 4 2 4 1 1.8 10.92
ResNet-34 Basic 1 3 6 4 8 6 12 3 6 1 3.6 8.58
ResNet-50 Bottle 1 3 9 4 12 6 18 3 9 1 3.8 7.13
ResNet-101 Bottle 1 3 9 4 12 23 69 3 9 1 7.6 6.44
ResNet-152 Bottle 1 3 9 8 24 36 108 3 9 1 11.3 5.94
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 96
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
Stage 1 Stage 2 Stage 3 Stage 4
Block 
type
Stem 
layers Blocks Layers Blocks Layers Blocks Layers Blocks Layers
FC 
layers GFLOP
ImageNet 
top-5 error
ResNet-18 Basic 1 2 4 2 4 2 4 2 4 1 1.8 10.92
ResNet-34 Basic 1 3 6 4 8 6 12 3 6 1 3.6 8.58
ResNet-50 Bottle 1 3 9 4 12 6 18 3 9 1 3.8 7.13
ResNet-101 Bottle 1 3 9 4 12 23 69 3 9 1 7.6 6.44
ResNet-152 Bottle 1 3 9 8 24 36 108 3 9 1 11.3 5.94
ResNet-50 is the same as ResNet-34, but replaces Basic blocks with Bottleneck Blocks.
This is a great baseline architecture for many tasks even today!
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 97
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
Error rates are 224x224 single-crop testing, reported by torchvision
Input
Softmax
3x3 conv, 64
7x7 conv, 64, / 2
FC 1000
Pool
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 64
3x3 conv, 128
3x3 conv, 128, / 2
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
3x3 conv, 128
..
.
3x3 conv, 512
3x3 conv, 512, /2
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
3x3 conv, 512
Pool
Stage 1 Stage 2 Stage 3 Stage 4
Block 
type
Stem 
layers Blocks Layers Blocks Layers Blocks Layers Blocks Layers
FC 
layers GFLOP
ImageNet 
top-5 error
ResNet-18 Basic 1 2 4 2 4 2 4 2 4 1 1.8 10.92
ResNet-34 Basic 1 3 6 4 8 6 12 3 6 1 3.6 8.58
ResNet-50 Bottle 1 3 9 4 12 6 18 3 9 1 3.8 7.13
ResNet-101 Bottle 1 3 9 4 12 23 69 3 9 1 7.6 6.44
ResNet-152 Bottle 1 3 9 8 24 36 108 3 9 1 11.3 5.94
Deeper ResNet-101 and ResNet-152 models are more 
accurate, but also more computationally heavy
Justin Johnson February 2, 2022
Residual Networks
Lecture 8 - 98
He et al, “Deep Residual Learning for Image Recognition”, CVPR 2016
- Able to train very deep networks
- Deeper networks do better than 
shallow networks (as expected)
- Swept 1st place in all ILSVRC and 
COCO 2015 competitions 
- Still widely used today!
Justin Johnson February 2, 2022
Improving Residual Networks: Block Design
Lecture 8 - 99
Conv
Batch Norm
ReLU
Conv
Batch Norm
ReLU
Batch Norm
ReLU
Conv
Batch Norm
ReLU
Conv
Original ResNet block “Pre-Activation” ResNet Block
He et al, ”Identity mappings in deep residual networks”, ECCV 2016
Note ReLU after residual:
Cannot actually learn 
identity function since 
outputs are nonnegative!
Note ReLU inside residual:
Can learn true identity 
function by setting Conv 
weights to zero!
Justin Johnson February 2, 2022
Improving Residual Networks: Block Design
Lecture 8 - 100
Conv
Batch Norm
ReLU
Conv
Batch Norm
ReLU
Batch Norm
ReLU
Conv
Batch Norm
ReLU
Conv
Original ResNet block “Pre-Activation” ResNet Block
He et al, ”Identity mappings in deep residual networks”, ECCV 2016
Slight improvement in accuracy
(ImageNet top-1 error)
ResNet-152: 21.3 vs 21.1
ResNet-200: 21.8 vs 20.7
Not actually used that much in 
practice
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 101
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 102
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
Inception-v4: Resnet + Inception!
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 103
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
VGG: Highest 
memory, most 
operations
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 104
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
GoogLeNet: 
Very efficient!
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 105
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
AlexNet: Low 
compute, lots 
of parameters
Justin Johnson February 2, 2022
Comparing Complexity
Lecture 8 - 106
Canziani et al, “An analysis of deep neural network models for practical applications”, 2017
ResNet: Simple design, 
moderate efficiency, 
high accuracy
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 107
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
Error Rate
Justin Johnson February 2, 2022
ImageNet Classification Challenge
Lecture 8 - 108
28.2
25.8
16.4
11.7
7.3 6.7
3.6 3 2.3
5.1
0
5
10
15
20
25
30
2010 2011 2012 2013 2014 2014 2015 2016 2017 Human
Shallow
8 layers 8 layers
19 
layers
22 
layers
152 
layers
152 
layers
152 
layers
Lin et al Sanchez & 
Perronnin
Krizhevsky et al 
(AlexNet)
Zeiler & 
Fergus
Simonyan & 
Zisserman (VGG)
Szegedy et al 
(GoogLeNet)
He et al 
(ResNet)
Russakovsky et al Shao et al Hu et al
(SENet)
CNN architectures 
have continued to 
evolve!
We will see more 
in Lecture 11
Error Rate
Justin Johnson February 2, 2022 Lecture 8 - 109
Next Time:
How to Train your CNN
- Activation functions
- Initialization
- Data preprocessing
- Data Augmentation
- Regularization
